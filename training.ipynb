{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-08-15T15:51:20.300711Z",
     "start_time": "2024-08-15T15:51:16.591692Z"
    }
   },
   "source": [
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision.datasets import CIFAR10\n",
    "from src.models.softmoe_lightning import LightningVitSoftMoE"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-15T15:51:21.591133Z",
     "start_time": "2024-08-15T15:51:20.309718Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_dataset = CIFAR10(root='./data', train=True, download=True,transform=ToTensor())\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_dataset = CIFAR10(root='./data', train=False, download=True,transform=ToTensor())\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=32, shuffle=False)"
   ],
   "id": "89aec602aa08558a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-15T15:51:21.674823Z",
     "start_time": "2024-08-15T15:51:21.667508Z"
    }
   },
   "cell_type": "code",
   "source": "len(train_loader)",
   "id": "ee5a02ab453d7f53",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1563"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-15T15:51:21.811139Z",
     "start_time": "2024-08-15T15:51:21.761546Z"
    }
   },
   "cell_type": "code",
   "source": [
    "image = train_dataset[0][0]\n",
    "image_size = image.shape[1]\n",
    "patch_size = image.shape[1] // 4\n",
    "channels = image.shape[0]\n",
    "dim = 64\n",
    "num_experts= 16\n",
    "num_slots = 8\n",
    "num_tokens = 16\n",
    "max_epochs = 30\n",
    "model = LightningVitSoftMoE(image_size=image_size, patch_size=patch_size, num_classes=10, \n",
    "                        dim=64, depth=6, heads=8, \n",
    "                        num_experts=num_experts, num_slots=num_slots, num_tokens=num_tokens, \n",
    "                        channels=3, dim_head=64,learning_rate=1e-5,warmup_steps=len(train_loader)* int(max_epochs * 0.1))"
   ],
   "id": "f4acb8c353b51a97",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "18c3e84a28ee4ee8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-15T15:51:27.214411Z",
     "start_time": "2024-08-15T15:51:21.840063Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tensorboard_logger = pl.loggers.TensorBoardLogger(\"logs\", name=\"ViTSoftMoE\")\n",
    "trainer = pl.Trainer(logger=tensorboard_logger, devices=torch.cuda.device_count(),accelerator=\"gpu\",max_epochs=max_epochs,precision=\"16-mixed\")\n",
    "\n",
    "trainer.fit(model, train_loader, val_loader)"
   ],
   "id": "20074c70c2ae92f1",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type             | Params | Mode \n",
      "---------------------------------------------------\n",
      "0 | model | ViTSoftMoE       | 2.4 M  | train\n",
      "1 | loss  | CrossEntropyLoss | 0      | train\n",
      "---------------------------------------------------\n",
      "2.4 M     Trainable params\n",
      "0         Non-trainable params\n",
      "2.4 M     Total params\n",
      "9.618     Total estimated model params size (MB)\n",
      "642       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fede_\\.conda\\envs\\MoE\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fede_\\.conda\\envs\\MoE\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   1%|          | 18/1563 [00:02<03:58,  6.49it/s, v_num=5]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Detected KeyboardInterrupt, attempting graceful shutdown ...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'exit' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "File \u001B[1;32m~\\.conda\\envs\\MoE\\Lib\\site-packages\\pytorch_lightning\\trainer\\call.py:47\u001B[0m, in \u001B[0;36m_call_and_handle_interrupt\u001B[1;34m(trainer, trainer_fn, *args, **kwargs)\u001B[0m\n\u001B[0;32m     46\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m trainer\u001B[38;5;241m.\u001B[39mstrategy\u001B[38;5;241m.\u001B[39mlauncher\u001B[38;5;241m.\u001B[39mlaunch(trainer_fn, \u001B[38;5;241m*\u001B[39margs, trainer\u001B[38;5;241m=\u001B[39mtrainer, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m---> 47\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m trainer_fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m     49\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m _TunerExitException:\n",
      "File \u001B[1;32m~\\.conda\\envs\\MoE\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:574\u001B[0m, in \u001B[0;36mTrainer._fit_impl\u001B[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001B[0m\n\u001B[0;32m    568\u001B[0m ckpt_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_checkpoint_connector\u001B[38;5;241m.\u001B[39m_select_ckpt_path(\n\u001B[0;32m    569\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mfn,\n\u001B[0;32m    570\u001B[0m     ckpt_path,\n\u001B[0;32m    571\u001B[0m     model_provided\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[0;32m    572\u001B[0m     model_connected\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlightning_module \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m    573\u001B[0m )\n\u001B[1;32m--> 574\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_run(model, ckpt_path\u001B[38;5;241m=\u001B[39mckpt_path)\n\u001B[0;32m    576\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mstopped\n",
      "File \u001B[1;32m~\\.conda\\envs\\MoE\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:981\u001B[0m, in \u001B[0;36mTrainer._run\u001B[1;34m(self, model, ckpt_path)\u001B[0m\n\u001B[0;32m    978\u001B[0m \u001B[38;5;66;03m# ----------------------------\u001B[39;00m\n\u001B[0;32m    979\u001B[0m \u001B[38;5;66;03m# RUN THE TRAINER\u001B[39;00m\n\u001B[0;32m    980\u001B[0m \u001B[38;5;66;03m# ----------------------------\u001B[39;00m\n\u001B[1;32m--> 981\u001B[0m results \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_run_stage()\n\u001B[0;32m    983\u001B[0m \u001B[38;5;66;03m# ----------------------------\u001B[39;00m\n\u001B[0;32m    984\u001B[0m \u001B[38;5;66;03m# POST-Training CLEAN UP\u001B[39;00m\n\u001B[0;32m    985\u001B[0m \u001B[38;5;66;03m# ----------------------------\u001B[39;00m\n",
      "File \u001B[1;32m~\\.conda\\envs\\MoE\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1025\u001B[0m, in \u001B[0;36mTrainer._run_stage\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1024\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mautograd\u001B[38;5;241m.\u001B[39mset_detect_anomaly(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_detect_anomaly):\n\u001B[1;32m-> 1025\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfit_loop\u001B[38;5;241m.\u001B[39mrun()\n\u001B[0;32m   1026\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\.conda\\envs\\MoE\\Lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:205\u001B[0m, in \u001B[0;36m_FitLoop.run\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    204\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mon_advance_start()\n\u001B[1;32m--> 205\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39madvance()\n\u001B[0;32m    206\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mon_advance_end()\n",
      "File \u001B[1;32m~\\.conda\\envs\\MoE\\Lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:363\u001B[0m, in \u001B[0;36m_FitLoop.advance\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    362\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_data_fetcher \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m--> 363\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mepoch_loop\u001B[38;5;241m.\u001B[39mrun(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_data_fetcher)\n",
      "File \u001B[1;32m~\\.conda\\envs\\MoE\\Lib\\site-packages\\pytorch_lightning\\loops\\training_epoch_loop.py:140\u001B[0m, in \u001B[0;36m_TrainingEpochLoop.run\u001B[1;34m(self, data_fetcher)\u001B[0m\n\u001B[0;32m    139\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 140\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39madvance(data_fetcher)\n\u001B[0;32m    141\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mon_advance_end(data_fetcher)\n",
      "File \u001B[1;32m~\\.conda\\envs\\MoE\\Lib\\site-packages\\pytorch_lightning\\loops\\training_epoch_loop.py:252\u001B[0m, in \u001B[0;36m_TrainingEpochLoop.advance\u001B[1;34m(self, data_fetcher)\u001B[0m\n\u001B[0;32m    251\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 252\u001B[0m             batch_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmanual_optimization\u001B[38;5;241m.\u001B[39mrun(kwargs)\n\u001B[0;32m    254\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbatch_progress\u001B[38;5;241m.\u001B[39mincrement_processed()\n",
      "File \u001B[1;32m~\\.conda\\envs\\MoE\\Lib\\site-packages\\pytorch_lightning\\loops\\optimization\\manual.py:94\u001B[0m, in \u001B[0;36m_ManualOptimization.run\u001B[1;34m(self, kwargs)\u001B[0m\n\u001B[0;32m     93\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m suppress(\u001B[38;5;167;01mStopIteration\u001B[39;00m):  \u001B[38;5;66;03m# no loop to break at this level\u001B[39;00m\n\u001B[1;32m---> 94\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39madvance(kwargs)\n\u001B[0;32m     95\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_restarting \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "File \u001B[1;32m~\\.conda\\envs\\MoE\\Lib\\site-packages\\pytorch_lightning\\loops\\optimization\\manual.py:114\u001B[0m, in \u001B[0;36m_ManualOptimization.advance\u001B[1;34m(self, kwargs)\u001B[0m\n\u001B[0;32m    113\u001B[0m \u001B[38;5;66;03m# manually capture logged metrics\u001B[39;00m\n\u001B[1;32m--> 114\u001B[0m training_step_output \u001B[38;5;241m=\u001B[39m call\u001B[38;5;241m.\u001B[39m_call_strategy_hook(trainer, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtraining_step\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;241m*\u001B[39mkwargs\u001B[38;5;241m.\u001B[39mvalues())\n\u001B[0;32m    115\u001B[0m \u001B[38;5;28;01mdel\u001B[39;00m kwargs  \u001B[38;5;66;03m# release the batch from memory\u001B[39;00m\n",
      "File \u001B[1;32m~\\.conda\\envs\\MoE\\Lib\\site-packages\\pytorch_lightning\\trainer\\call.py:319\u001B[0m, in \u001B[0;36m_call_strategy_hook\u001B[1;34m(trainer, hook_name, *args, **kwargs)\u001B[0m\n\u001B[0;32m    318\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m trainer\u001B[38;5;241m.\u001B[39mprofiler\u001B[38;5;241m.\u001B[39mprofile(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m[Strategy]\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtrainer\u001B[38;5;241m.\u001B[39mstrategy\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mhook_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m--> 319\u001B[0m     output \u001B[38;5;241m=\u001B[39m fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    321\u001B[0m \u001B[38;5;66;03m# restore current_fx when nested context\u001B[39;00m\n",
      "File \u001B[1;32m~\\.conda\\envs\\MoE\\Lib\\site-packages\\pytorch_lightning\\strategies\\strategy.py:390\u001B[0m, in \u001B[0;36mStrategy.training_step\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m    389\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_redirection(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlightning_module, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtraining_step\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m--> 390\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlightning_module\u001B[38;5;241m.\u001B[39mtraining_step(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\Desktop\\MoE\\VisionSoftMoE\\src\\models\\softmoe_lightning.py:46\u001B[0m, in \u001B[0;36mLightningVitSoftMoE.training_step\u001B[1;34m(self, batch, batch_idx)\u001B[0m\n\u001B[0;32m     45\u001B[0m loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mloss(logits,y)\n\u001B[1;32m---> 46\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmanual_backward(loss,retain_graph\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m     47\u001B[0m opt\u001B[38;5;241m.\u001B[39mstep()\n",
      "File \u001B[1;32m~\\.conda\\envs\\MoE\\Lib\\site-packages\\pytorch_lightning\\core\\module.py:1082\u001B[0m, in \u001B[0;36mLightningModule.manual_backward\u001B[1;34m(self, loss, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1081\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_verify_is_manual_optimization(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmanual_backward\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m-> 1082\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrainer\u001B[38;5;241m.\u001B[39mstrategy\u001B[38;5;241m.\u001B[39mbackward(loss, \u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\.conda\\envs\\MoE\\Lib\\site-packages\\pytorch_lightning\\strategies\\strategy.py:212\u001B[0m, in \u001B[0;36mStrategy.backward\u001B[1;34m(self, closure_loss, optimizer, *args, **kwargs)\u001B[0m\n\u001B[0;32m    210\u001B[0m closure_loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprecision_plugin\u001B[38;5;241m.\u001B[39mpre_backward(closure_loss, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlightning_module)\n\u001B[1;32m--> 212\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprecision_plugin\u001B[38;5;241m.\u001B[39mbackward(closure_loss, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlightning_module, optimizer, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    214\u001B[0m closure_loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprecision_plugin\u001B[38;5;241m.\u001B[39mpost_backward(closure_loss, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlightning_module)\n",
      "File \u001B[1;32m~\\.conda\\envs\\MoE\\Lib\\site-packages\\pytorch_lightning\\plugins\\precision\\precision.py:72\u001B[0m, in \u001B[0;36mPrecision.backward\u001B[1;34m(self, tensor, model, optimizer, *args, **kwargs)\u001B[0m\n\u001B[0;32m     61\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"Performs the actual backpropagation.\u001B[39;00m\n\u001B[0;32m     62\u001B[0m \n\u001B[0;32m     63\u001B[0m \u001B[38;5;124;03mArgs:\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     70\u001B[0m \n\u001B[0;32m     71\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m---> 72\u001B[0m model\u001B[38;5;241m.\u001B[39mbackward(tensor, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\.conda\\envs\\MoE\\Lib\\site-packages\\pytorch_lightning\\core\\module.py:1101\u001B[0m, in \u001B[0;36mLightningModule.backward\u001B[1;34m(self, loss, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1100\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1101\u001B[0m     loss\u001B[38;5;241m.\u001B[39mbackward(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\.conda\\envs\\MoE\\Lib\\site-packages\\torch\\_tensor.py:521\u001B[0m, in \u001B[0;36mTensor.backward\u001B[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[0;32m    512\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[0;32m    513\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[0;32m    514\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    519\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[0;32m    520\u001B[0m     )\n\u001B[1;32m--> 521\u001B[0m torch\u001B[38;5;241m.\u001B[39mautograd\u001B[38;5;241m.\u001B[39mbackward(\n\u001B[0;32m    522\u001B[0m     \u001B[38;5;28mself\u001B[39m, gradient, retain_graph, create_graph, inputs\u001B[38;5;241m=\u001B[39minputs\n\u001B[0;32m    523\u001B[0m )\n",
      "File \u001B[1;32m~\\.conda\\envs\\MoE\\Lib\\site-packages\\torch\\autograd\\__init__.py:289\u001B[0m, in \u001B[0;36mbackward\u001B[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[0;32m    286\u001B[0m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[0;32m    287\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[0;32m    288\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[1;32m--> 289\u001B[0m _engine_run_backward(\n\u001B[0;32m    290\u001B[0m     tensors,\n\u001B[0;32m    291\u001B[0m     grad_tensors_,\n\u001B[0;32m    292\u001B[0m     retain_graph,\n\u001B[0;32m    293\u001B[0m     create_graph,\n\u001B[0;32m    294\u001B[0m     inputs,\n\u001B[0;32m    295\u001B[0m     allow_unreachable\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[0;32m    296\u001B[0m     accumulate_grad\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[0;32m    297\u001B[0m )\n",
      "File \u001B[1;32m~\\.conda\\envs\\MoE\\Lib\\site-packages\\torch\\autograd\\graph.py:768\u001B[0m, in \u001B[0;36m_engine_run_backward\u001B[1;34m(t_outputs, *args, **kwargs)\u001B[0m\n\u001B[0;32m    767\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 768\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m Variable\u001B[38;5;241m.\u001B[39m_execution_engine\u001B[38;5;241m.\u001B[39mrun_backward(  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[0;32m    769\u001B[0m         t_outputs, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs\n\u001B[0;32m    770\u001B[0m     )  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[0;32m    771\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[5], line 4\u001B[0m\n\u001B[0;32m      1\u001B[0m tensorboard_logger \u001B[38;5;241m=\u001B[39m pl\u001B[38;5;241m.\u001B[39mloggers\u001B[38;5;241m.\u001B[39mTensorBoardLogger(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlogs\u001B[39m\u001B[38;5;124m\"\u001B[39m, name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mViTSoftMoE\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m      2\u001B[0m trainer \u001B[38;5;241m=\u001B[39m pl\u001B[38;5;241m.\u001B[39mTrainer(logger\u001B[38;5;241m=\u001B[39mtensorboard_logger, devices\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mcuda\u001B[38;5;241m.\u001B[39mdevice_count(),accelerator\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgpu\u001B[39m\u001B[38;5;124m\"\u001B[39m,max_epochs\u001B[38;5;241m=\u001B[39mmax_epochs,precision\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m16-mixed\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m----> 4\u001B[0m trainer\u001B[38;5;241m.\u001B[39mfit(model, train_loader, val_loader)\n",
      "File \u001B[1;32m~\\.conda\\envs\\MoE\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:538\u001B[0m, in \u001B[0;36mTrainer.fit\u001B[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001B[0m\n\u001B[0;32m    536\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mstatus \u001B[38;5;241m=\u001B[39m TrainerStatus\u001B[38;5;241m.\u001B[39mRUNNING\n\u001B[0;32m    537\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtraining \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m--> 538\u001B[0m call\u001B[38;5;241m.\u001B[39m_call_and_handle_interrupt(\n\u001B[0;32m    539\u001B[0m     \u001B[38;5;28mself\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n\u001B[0;32m    540\u001B[0m )\n",
      "File \u001B[1;32m~\\.conda\\envs\\MoE\\Lib\\site-packages\\pytorch_lightning\\trainer\\call.py:64\u001B[0m, in \u001B[0;36m_call_and_handle_interrupt\u001B[1;34m(trainer, trainer_fn, *args, **kwargs)\u001B[0m\n\u001B[0;32m     62\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(launcher, _SubprocessScriptLauncher):\n\u001B[0;32m     63\u001B[0m         launcher\u001B[38;5;241m.\u001B[39mkill(_get_sigkill_signal())\n\u001B[1;32m---> 64\u001B[0m     exit(\u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m     66\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m exception:\n\u001B[0;32m     67\u001B[0m     _interrupt(trainer, exception)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'exit' is not defined"
     ]
    }
   ],
   "execution_count": 5
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
